继上次的hadoop环境搭建，先追加安装spark集群。以下操作在master主机上完成。spark依赖于scala环境，所以首先安装scala。

#### 安装scala ####
下载scala安装包```scala-2.11.0.tgz```，解压到```/usr/local/etc/```路径下。
配置环境变量：编辑```/etc/profile```文件，在最后面添加如下配置：
```bash
export SCALA_HOME=/usr/local/etc/scala-2.11.0
export PATH=$PATH:${SCALA_HOME}/bin
```
编辑并保存完成```etc/profile```文件之后，执行：
```bash
hadoop@master:~$ source /etc/profile # 使生效
hadoop@master:~$ scala -version
Scala code runner version 2.11.0 -- Copyright 2002-2013, LAMP/EPFL
```

#### 安装Spark ####

下载Spark安装包```spark-1.6.1-bin-hadoop2.6.tgz```，解压到```/usr/local/etc/```路径下。
配置环境变量：编辑```/etc/profile```文件，在最后面添加如下配置：
```
export SPARK_HOME=/usr/local/etc/spark-1.6.1-bin-hadoop2.6
export PATH=$PATH:${SPARK_HOME}/bin:${SPARK_HOME}/sbin
```
编辑并保存完成```etc/profile```文件之后，执行：
```bash
hadoop@master:~$ source /etc/profile # 使生效
```
>这里需要注意一下，因为之前在配置Hadoop环境的时候，在hadoop中已经存在```start-all.sh```以及```stop-all.sh```这两个文件，可以更改spark中```sbin/```的对应文件的名称以避免冲突，甚至还有其他重名的文件，在使用过程中还需多加注意。
>
>这里我将spark中的这两个文件改名为对应的```spark-start-all.sh```和```spark-stop-all.sh```。

---
接下来配置spark集群环境
转移到spark根目录下的```conf```文件夹中。
```bash
hadoop@master:/usr/local/etc/spark-1.6.1-bin-hadoop2.6/conf$ ls
docker.properties.template  metrics.properties.template   spark-env.sh.template
fairscheduler.xml.template  slaves.template
log4j.properties.template   spark-defaults.conf.template
```
复制一份```slaves.template```文件，其名为```slaves```：
```
hadoop@master:/usr/local/etc/spark-1.6.1-bin-hadoop2.6/conf$ sudo cp slaves.template slaves
```
编辑```slaves```文件，在后面追加：
```bash
master # 删除这里的 localhost，并改为 master
slave01	# 在hadoop环境配置时已经配置好了两个从属主机的名称，对应的是 192.168.1.201 
slave02	# 192.168.1.202
```
复制一份```spark-env.sh.template```文件，其名为```spark-env.sh```：
```bash
hadoop@master:/usr/local/etc/spark-1.6.1-bin-hadoop2.6/conf$ sudo cp spark-env.sh.template spark-env.sh
```
编辑```spark-env.sh```文件，在后面追加：
```bash
export JAVA_HOME=/usr/lib/java/jdk1.8.0_45
export SCALA_HOME=/usr/local/etc/scala-2.11.0
```

>完成以上操作之后，将该master上的所有操作在slaves（slave01,slave02）主机上也进行同样的操作。确保环境变量，spark的配置信息等，在三台主机上的信息一致。

完成所有操作之后，使用```spark-start-all.sh```启动spark。在浏览器地址栏输入："master:8080"


#### 总结一下
>1、下载文件”scala-2.11.0.tgz“、”spark-1.6.1-bin-hadoop2.6.tgz“
>2、将这两个文件解压到```/usr/local/etc/```路径下
>3、在```/etc/profile```文件中添加scala和spark的环境变量
>4、配置spark的conf，配置```slaves```文件添加从属机器，配置```spark-env.sh```添加两个环境变量。
>5、三台主机（master、slave01、slave02）做相同的配置